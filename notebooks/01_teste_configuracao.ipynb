{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4d67c9d0-97d5-4d72-b248-136be9a9185e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Notebook de Teste: Verificação do Ambiente Databricks para Projeto MLSecOps\n",
    "# Autor: Luana Gonçalves\n",
    "# Data: 5 de abril de 2025\n",
    "\n",
    "# 1. Verificação de versões de bibliotecas essenciais\n",
    "print(\"=== Verificando versões de bibliotecas essenciais ===\")\n",
    "\n",
    "import sys\n",
    "print(f\"Python version: {sys.version}\")\n",
    "\n",
    "# Verificando PySpark\n",
    "try:\n",
    "    import pyspark\n",
    "    print(f\"PySpark version: {pyspark.__version__}\")\n",
    "except ImportError:\n",
    "    print(\"⚠️ PySpark não está disponível\")\n",
    "\n",
    "# Verificando Delta Lake\n",
    "try:\n",
    "    from delta import *\n",
    "    print(f\"Delta Lake está disponível\")\n",
    "except ImportError:\n",
    "    print(\"⚠️ Delta Lake não está disponível\")\n",
    "\n",
    "# Verificando bibliotecas para ML\n",
    "try:\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    print(f\"Pandas version: {pd.__version__}\")\n",
    "    print(f\"NumPy version: {np.__version__}\")\n",
    "except ImportError as e:\n",
    "    print(f\"⚠️ Erro ao importar bibliotecas de análise de dados: {e}\")\n",
    "\n",
    "try:\n",
    "    import sklearn\n",
    "    print(f\"Scikit-learn version: {sklearn.__version__}\")\n",
    "except ImportError:\n",
    "    print(\"⚠️ Scikit-learn não está disponível\")\n",
    "\n",
    "try:\n",
    "    import mlflow\n",
    "    print(f\"MLflow version: {mlflow.__version__}\")\n",
    "except ImportError:\n",
    "    print(\"⚠️ MLflow não está disponível\")\n",
    "\n",
    "# 2. Teste de conexão com Spark\n",
    "print(\"\\n=== Testando conexão com Spark ===\")\n",
    "try:\n",
    "    # Criando um DataFrame de teste\n",
    "    data = [(\"1\", \"Transação Normal\", 500.0, False),\n",
    "            (\"2\", \"Transação Suspeita\", 9999.0, True)]\n",
    "    \n",
    "    columns = [\"id\", \"tipo\", \"valor\", \"fraude\"]\n",
    "    \n",
    "    df = spark.createDataFrame(data, columns)\n",
    "    print(\"DataFrame de teste criado com sucesso:\")\n",
    "    df.show()\n",
    "    \n",
    "    # Testando algumas operações Spark\n",
    "    print(\"\\nOperações básicas com Spark:\")\n",
    "    df_filtrado = df.filter(df.fraude == True)\n",
    "    print(\"Filtrando transações fraudulentas:\")\n",
    "    df_filtrado.show()\n",
    "    \n",
    "    # Testando SQL\n",
    "    print(\"\\nRegistrando como tabela temporária e executando SQL:\")\n",
    "    df.createOrReplaceTempView(\"transacoes_teste\")\n",
    "    spark.sql(\"SELECT * FROM transacoes_teste WHERE valor > 1000\").show()\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"⚠️ Erro na conexão com Spark: {e}\")\n",
    "\n",
    "# 3. Teste de escrita e leitura com Delta Lake\n",
    "print(\"\\n=== Testando Delta Lake ===\")\n",
    "try:\n",
    "    # Caminho temporário para teste\n",
    "    temp_path = \"/tmp/delta_test\"\n",
    "    \n",
    "    # Salvando o DataFrame como tabela Delta\n",
    "    print(f\"Salvando dados como Delta no caminho: {temp_path}\")\n",
    "    df.write.format(\"delta\").mode(\"overwrite\").save(temp_path)\n",
    "    \n",
    "    # Lendo dados do Delta\n",
    "    print(\"Lendo dados da tabela Delta:\")\n",
    "    delta_df = spark.read.format(\"delta\").load(temp_path)\n",
    "    delta_df.show()\n",
    "    \n",
    "    # Testando operações do Delta Lake\n",
    "    print(\"\\nVerificando histórico de versões da tabela Delta:\")\n",
    "    delta_table = DeltaTable.forPath(spark, temp_path)\n",
    "    delta_table.history().show(5, truncate=False)\n",
    "    \n",
    "    print(\"\\nDelta Lake está funcionando corretamente!\")\n",
    "except Exception as e:\n",
    "    print(f\"⚠️ Erro ao testar Delta Lake: {e}\")\n",
    "\n",
    "# 4. Teste de criação de banco de dados e tabelas para a arquitetura Medallion\n",
    "print(\"\\n=== Testando criação de bancos de dados para arquitetura Medallion ===\")\n",
    "try:\n",
    "    # Criando os bancos de dados\n",
    "    spark.sql(\"CREATE DATABASE IF NOT EXISTS bronze\")\n",
    "    spark.sql(\"CREATE DATABASE IF NOT EXISTS silver\")\n",
    "    spark.sql(\"CREATE DATABASE IF NOT EXISTS gold\")\n",
    "    \n",
    "    # Verificando se os bancos foram criados\n",
    "    print(\"Bancos de dados disponíveis:\")\n",
    "    spark.sql(\"SHOW DATABASES\").show()\n",
    "    \n",
    "    # Criando uma tabela de teste no banco bronze\n",
    "    print(\"\\nCriando tabela de teste na camada Bronze:\")\n",
    "    df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"bronze.transacoes_teste\")\n",
    "    \n",
    "    # Verificando a tabela criada\n",
    "    print(\"Tabelas no banco de dados bronze:\")\n",
    "    spark.sql(\"SHOW TABLES IN bronze\").show()\n",
    "    \n",
    "    # Lendo a tabela\n",
    "    print(\"Lendo dados da tabela bronze.transacoes_teste:\")\n",
    "    spark.sql(\"SELECT * FROM bronze.transacoes_teste\").show()\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"⚠️ Erro ao testar criação de bancos de dados: {e}\")\n",
    "\n",
    "# 5. Informações sobre o ambiente Spark\n",
    "print(\"\\n=== Informações sobre o ambiente Spark ===\")\n",
    "try:\n",
    "    # Configurações do Spark\n",
    "    print(\"Configurações do Spark:\")\n",
    "    for config in [\"spark.app.name\", \"spark.driver.memory\", \n",
    "                  \"spark.executor.memory\", \"spark.executor.cores\"]:\n",
    "        try:\n",
    "            value = spark.conf.get(config)\n",
    "            print(f\"  {config}: {value}\")\n",
    "        except:\n",
    "            print(f\"  {config}: Não disponível\")\n",
    "    \n",
    "    # Informações sobre o cluster\n",
    "    print(\"\\nInformações sobre o ambiente:\")\n",
    "    sc_conf = spark.sparkContext.getConf()\n",
    "    master = sc_conf.get(\"spark.master\")\n",
    "    app_id = sc_conf.get(\"spark.app.id\")\n",
    "    \n",
    "    print(f\"  Spark Master: {master}\")\n",
    "    print(f\"  Application ID: {app_id}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"⚠️ Erro ao obter informações do ambiente: {e}\")\n",
    "\n",
    "print(\"\\n=== Verificação de ambiente concluída ===\")\n",
    "print(\"✅ O ambiente está pronto para o desenvolvimento do projeto de detecção de fraudes Pix!\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "01_teste_configuracao",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
